)
# Gather method - reshaping data from wide format to long format
# Here, Face & ResponseTime are the names of the columns
# Face will take values like Face.1, Face.2
# ResponseTime will store the value of each Face.1 or Face.2 or Face.3
long <- wide %>% gather(Face, ResponseTime, Face.1:Face.3)
# separate method - splits a single column into multiple columns
long_seperate <- long %>% separate(Face, c("Target", "Number"))
# Unite method - combines multiple columns into single column
long_unite <- long_seperate %>% unite(Face, Target, Number, sep=".")
View(long_unite)
View(long_unite)
# spread method - takes two columns (key & value) and spreads in to multiple columns
# makes data wider
back_to_wide <- long_unite %>% spread(Face, ResponseTime)
View(back_to_wide)
library(datasets)
plot(ChickWeight)
# base graphics
library(MASS)
plot(UScereal$sugars, UScereal$calories)
title("plot(UScereal$sugars, UScereal$calories)")
x <- UScereal$sugars
y <- UScereal$calories
library(grid)
# Grid graphics
pushViewport(plotViewport())
pushViewport(dataViewport(x, y))
grid.rect()
grid.xaxis()
grid.yaxis()
grid.points(x, y)
grid.text("UScereal$calories", x = unit(-3, "lines"), rot = 90)
# Grid graphics
pushViewport(plotViewport())
pushViewport(dataViewport(x, y))
grid.rect()
grid.xaxis()
grid.yaxis()
grid.points(x, y)
grid.text("UScereal$calories", x = unit(-3, "lines"), rot = 90)
grid.text("UScereal$sugars", y = unit(-3, "lines"), rot = 0)
plot(UScereal$sugars, UScereal$calories)
title("plot(UScereal$sugars, UScereal$calories)")
x <- UScereal$sugars
y <- UScereal$calories
library(grid)
# Grid graphics
pushViewport(plotViewport())
pushViewport(dataViewport(x, y))
grid.rect()
grid.xaxis()
grid.yaxis()
grid.points(x, y)
grid.text("UScereal$calories", x = unit(-3, "lines"), rot = 90)
grid.text("UScereal$sugars", y = unit(-3, "lines"), rot = 0)
grid.circle()
# Grid graphics
pushViewport(plotViewport())
pushViewport(dataViewport(x, y))
grid.circle()
grid.xaxis()
grid.yaxis()
grid.points(x, y)
# base graphics
library(MASS)
plot(UScereal$sugars, UScereal$calories)
title("plot(UScereal$sugars, UScereal$calories)")
# 3D pie charts
install.packages("plotrix")
# 3D pie charts
library(plotrix)
x <- c(33, 45, 70, 110)
lbl <- c("Soap", "Detergent", "Oil", "Shampoo")
pie3D(x, labels=lbl, explore=0.1, main="Pie chart of countries")
Sys.setenv(JAVA_HOME = "C:\\Program Files\\Java\\jdk-18.0.1.1")
Sys.getenv("JAVA_HOME")
library(rpart)
library(caret)
library(rpart.plot)
library(dplyr)
library(data.tree)
library(caTools)
install.packages("ElemStatLearn")
R -version
R.version()
R.Version()
R --version
install.packages("installr")
library(installr)
updateR()
# Import Data
data <- read.csv("Salaries.csv")
setwd("D:/Study/Learning/R/ML Course/Hierarchical Clustering")
# Import Data
data <- read.csv("Salaries.csv")
# Selecting only required columns
data <- select(data, -c("Status", "Notes", "Year", "Benefits", "Id", "EmployeeName", "Agency"))
# Selecting only required columns
data <- select(data, -c("Status", "Notes", "Year", "Benefits", "Id", "EmployeeName", "Agency"))
# Library imports
library(caTools)
# Selecting only required columns
data <- select(data, -c("Status", "Notes", "Year", "Benefits", "Id", "EmployeeName", "Agency"))
library(dplyr)
# Selecting only required columns
data <- select(data, -c("Status", "Notes", "Year", "Benefits", "Id", "EmployeeName", "Agency"))
# Removing na values
na.omit(data)
View(data)
# plotting
plot(JobTitle ~ BasePay, data = data)
# Factoring job title
data$JobTitle <- as.factor(data$JobTitle)
# plotting
plot(JobTitle ~ BasePay, data = data[100,])
# plotting
plot(JobTitle ~ BasePay, data = data[100])
# plotting
plot(JobTitle ~ BasePay, data = data[5,])
datasets()
dataset()
data()
head(USAccDeaths)
head(UKgas)
head(iris)
head(UScitiesD)
head(airmiles)
head(airq)
head(airquality)
print(airquality)
head(cars)
head(mtcars)
head(iris3)
head(presidents)
head(trees)
head(women)
head(uspop)
head(starwars)
head(starwars)
head(storms)
head(storms)
rnorm(10, mean = 10000, sd = 1)
rnorm(10, mean = 10000, sd = 2)
rnorm(10, mean = 10000, sd = 5)
rnorm(10, mean = 10000, sd = 10)
rnorm(10, mean = 10000, sd = 100)
rnorm(10, mean = 10000, sd = 10000)
rnorm(10, mean = 10000, sd = 5000)
typeof(rnorm(10, mean = 10000, sd = 5000))
class(rnorm(10, mean = 10000, sd = 5000))
test <- rnorm(10, mean = 10000, sd = 5000)
test[1]
test[2]
install.packages("factoextra")
library(factoextra)
iris
iris.labels = iris$Species
table(iris.labels)
iris_data <- iris[1:4]
# Scaling the data
iris_data_scale = scale(iris_data)
# Distance
iris_data <- dist(iris_data_scale)
iris_data
# Calculate the number of clusters we need
# wss - within sum of squares
fviz_nbclust(iris_data_scale, kmeans, method = "wss") + labs(subtitle = "Elbow method")
# Kmeans
km.out <- kmeans(iris_data_scale, centers = 3)
print(km.out)
# Visualize the clustering algorithm results
print(km.out$cluster)
rownames(iris_data_scale) <- paste(iris$Species, 1:dim(iris)[1], sep = "_")
# Visualize the clustering algorithm results
clusters = km.out$cluster
km.out
iris_data_scale
rownames(iris_data_scale) <- paste(iris$Species, 1:dim(iris)[1], sep = "_")
iris_data_scale
fviz_cluster(list(data=iris_data_scale, cluster = km.clusters))
fviz_cluster(list(data=iris_data_scale, cluster = clusters))
iris
iris.labels = iris$Species
rm(list = ls())
iris.labels = iris$Species
iris.labels
table(iris.labels)
iris_data_scale = scale(iris_data)
# Preparing unlabeled data
iris_data <- iris[1:4]
# Scaling the data
iris_data_scale = scale(iris_data)
dist?
?dist
dist(iris_data_scale)
?dist
typeof(iris_data_scale)
class(iris_data_scale)
iris_data_scale
myData <- iris[1:4]
dist(myData)
scale(myData)
?scale
iris_data_scale
print(km.out)
# Finding the distance measure using euclidean method
iris_data <- dist(iris_data_scale, method = "euclidean")
iris_data
# Finding the distance measure using euclidean method
iris_data <- dist(iris_data_scale, method = "euclidean")
# Calculate the number of clusters we need
# wss - within sum of squares
fviz_nbclust(iris_data_scale, kmeans, method = "wss") + labs(subtitle = "Elbow method")
fviz_nbclust(iris_data_scale, kmeans, method = "wss")
?fbviz_nbclust
?fviz_nbclust
# Calculate the number of clusters we need
# wss - within sum of squares
fviz_nbclust(iris_data_scale, kmeans, method = "gap_stat") + labs(subtitle = "Elbow method")
# Calculate the number of clusters we need
# wss - within sum of squares
fviz_nbclust(iris_data_scale, kmeans, method = "silhoutte") + labs(subtitle = "Elbow method")
?fviz_nbclust
# Calculate the number of clusters we need
# wss - within sum of squares
fviz_nbclust(iris_data_scale, kmeans, method = "silhouette") + labs(subtitle = "Elbow method")
# Calculate the number of clusters we need
# wss - within sum of squares, silhouette, gap_stat
fviz_nbclust(iris_data_scale, kmeans, method = "gap_stat") + labs(subtitle = "Elbow method")
# Kmeans
km.out <- kmeans(iris_data_scale, centers = 2)
print(km.out)
# Visualize the clustering algorithm results
clusters = km.out$cluster
dim(iris)
dim(iris)[1]
names(iris_data_scale)
iris_data_scale
rownames(iris_data_scale) <- paste(iris$Species, 1:dim(iris)[1], sep = "_")
iris_data_scale
list(data=iris_data_scale, cluster = clusters)
?fviz_cluster
fviz_cluster(list(data=iris_data_scale, cluster = clusters))
iris[1:4]
iris[1:4][1,]
test <- predict(km.out, iris[1:4][1,], class = "response")
# Kmeans
km.out <- kmeans(iris_data_scale, centers = 3)
print(km.out)
# Visualize the clustering algorithm results
clusters = km.out$cluster
rownames(iris_data_scale) <- paste(iris$Species, 1:dim(iris)[1], sep = "_")
fviz_cluster(list(data=iris_data_scale, cluster = clusters))
test <- predict(km.out, iris[1:4][1,], class = "response")
groups <- cutree(km.out$cluster, k = 3)
groups <- cutree(iris_data_scale, k = 3)
knnClust = knn(train = iris_data_scale, test = iris[1,4][1,], k = 1)
library(factoextra)
install.package("scorecard")
install.packages("scorecard")
library(class)
library(scorecard)
library(caTools)
# Splitting data
split <- sample.split(iris, SplitRatio = 0.7)
train <- subset(iris, split == TRUE)
test <- subset(iris, split == FALSE)
iris[,-5]
# Calculating distance
distIris <- dist(scale(train[1, 4]))
hc_iris <- hclust(distIris, method = "complete")
hc_iris <- hclust(distIris, method = "ward.D2")
distIris
train
# Calculating distance
distIris <- dist(scale(train[1, 4]), method = "euclidean")
hc_iris <- hclust(distIris, method = "complete")
distIris
scale(train[1, 4]
scale(train[1, 4])
scale(train[1,4])
train[1,4]
train
class(train)
train[1]
train[1, 4]
train
iris[1, 4]
# Calculating distance
distIris <- dist(scale(train[1 : 4]), method = "euclidean")
distIris
hc_iris <- hclust(distIris, method = "complete")
hc_iris
groups <- cutree(hc_iris, k = 3)
table(groups)
knnClust <- knn(train = train, test = test, k = 1, cl = groups)
knnClust <- knn(train = train, test = test, k = 2, cl = groups)
knnClust <- knn(train = train, test = test, k = 3, cl = groups)
test
knnClust <- knn(train = train[1:4], test = test[1:4], k = 3, cl = groups)
knnClust
plot(hc_iris)
plot(knnClust)
p1 <- fviz_cluster(list(data = train[,-5], cluster = groups), stand = F) + xlim(-11.2,-4.8) + ylim(-3,3) + ggtitle("train")
p2 <- fviz_cluster(list(data = test[,-5], cluster = knnClust),stand = F) + xlim(-11.2,-4.8) + ylim(-3,3) + ggtitle("test")
gridExtra::grid.arrange(p1, p2, nrow = 2)
p1 <- fviz_cluster(list(data = train[,-5], cluster = groups), stand = T) + xlim(-11.2,-4.8) + ylim(-3,3) + ggtitle("train")
p2 <- fviz_cluster(list(data = test[,-5], cluster = knnClust),stand = T) + xlim(-11.2,-4.8) + ylim(-3,3) + ggtitle("test")
gridExtra::grid.arrange(p1, p2, nrow = 2)
p2 <- fviz_cluster(list(data = test[,-5], cluster = knnClust),stand = F) + xlim(-11.2,-4.8) + ylim(-3,3) + ggtitle("test")
p1 <- fviz_cluster(list(data = train[,-5], cluster = groups), stand = F) + xlim(-11.2,-4.8) + ylim(-3,3) + ggtitle("train")
gridExtra::grid.arrange(p1, p2, nrow = 2)
# Groups
groups <- cutree(hc_iris, k = 3)
# Testing the model
knnClust <- knn(train = train[1:4], test = test[1:4], k = 1, cl = groups)
sample(1:nrow(iris), size = 1)
sample(1:nrow(iris))
sample(1:nrow(iris))
# Predicting new data
randomData <- iris[sample(1:nrow(iris), size = 1), 4]
# Predicting new data
randomData <- iris[sample(1:nrow(iris), size = 1) : 4]
sample(1:nrow(iris), size = 1)
iris[24, 4]
sample(1:nrow(iris), size = 1)
iris[24 : 4]
iris[1:4][24,]
# Predicting new data
randomData <- iris[1:4][sample(1:nrow(iris), size = 1),]
predictedData <- knn(train = train[1:4], test = randomData, k = 1, cl = groups)
predictedData
randomData
iris[37, ]
knnClust
train
train[1:4]
rownames(train[1:4]) <- paste(train$Species, 1:dim(train)[1], sep = "_")
train
rownames(train) <- paste(train$Species, 1:dim(train)[1], sep = "_")
train
# Predicting new data
randomData <- iris[1:4][sample(1:nrow(iris), size = 1),]
predictedData <- knn(train = train[1:4], test = randomData, k = 1, cl = groups)
train = train[1:4]
rownames(train) <- paste(iris$Species, 1:dim(train)[1], sep = "_")
rownames(train) <- paste(iris$Species, 1:dim(iris)[1], sep = "_")
train
# Predicting new data
randomData <- iris[1:4][sample(1:nrow(iris), size = 1),]
predictedData <- knn(train = train, test = randomData, k = 1, cl = groups)
groups
hc_iris
distIris
randomData
iris[144,]
predictedData
library(factoextra)
library(class)
library(scorecard)
library(caTools)
# data
iris
# Splitting data
split <- sample.split(iris, SplitRatio = 0.7)
# splitting data into training and testing
data <- subset(iris[1:4], split == TRUE)
# Calculating distance
distData <- dist(scale(data), method = "euclidean")
# Creating clustering model
hc_iris <- hclust(distData, method = "complete")
# Groups
groups <- cutree(hc_iris, k = 3)
# plotting the data
p1 <- fviz_cluster(list(data = data[,-5], cluster = groups), stand = F) + xlim(-11.2,-4.8) + ylim(-3,3) + ggtitle("Data")
gridExtra::grid.arrange(p1)
# Predicting new data
randomData <- iris[1:4][sample(1:nrow(iris), size = 1),]
predictedData <- knn(train = data, test = randomData, k = 1, cl = groups)
rm(list = ls())
# Retrieve all the species
iris.labels = iris$Species
# Plotting in the table
table(iris.labels)
# Preparing unlabeled data
iris_data <- iris[1:4]
# Scaling the data
iris_data_scale = scale(iris_data)
# Finding the distance measure using euclidean method
iris_data <- dist(iris_data_scale, method = "euclidean")
# Calculate the number of clusters we need
# wss - within sum of squares, silhouette, gap_stat
fviz_nbclust(iris_data_scale, kmeans, method = "gap_stat") + labs(subtitle = "Elbow method")
# Kmeans
km.out <- kmeans(iris_data_scale, centers = 3)
print(km.out)
# Visualize the clustering algorithm results
clusters = km.out$cluster
iris_data_scales
iris_data_scale
rownames(iris_data_scale) <- paste(iris$Species, 1:dim(iris)[1], sep = "_")
fviz_cluster(list(data=iris_data_scale, cluster = clusters))
rm(list = ls())
# data
data = iris[1:4]
# Scaled Data
scaledData <- scale(data)
# Calculating distance
distData <- dist(scaledData, method = "euclidean")
# Creating clustering model
hc_iris <- hclust(distData, method = "complete")
# Row names
rownames(scaledData) <- paste(iris$Species, 1:dim(iris)[1], sep = "_")
scaledData
hc_iris
labels(hc_iris) <- paste(iris$Species, 1:dim(iris)[1], sep = "_")
hc_iris$labels <- paste(iris$Species, 1:dim(iris)[1], sep = "_")
# plotting the data
p1 <- fviz_cluster(list(data = scaledData, cluster = groups), stand = F) + xlim(-11.2,-4.8) + ylim(-3,3) + ggtitle("Data")
# Groups
groups <- cutree(hc_iris, k = 3)
fviz_cluster(list(data=scaledData, cluster = groups))
levels(iris)
iris
levels(iris$Species)
hc_iris$labels <- levles(iris$Species)
hc_iris$labels <- levels(iris$Species)
# plotting the data
fviz_cluster(list(data = scaledData, cluster = groups), stand = F) + xlim(-11.2,-4.8) + ylim(-3,3) + ggtitle("Data")
scaledData
hc_iris$labels <- paste(iris$Species, 1:dim(iris)[1], sep = "_")
# plotting the data
fviz_cluster(list(data = scaledData, cluster = groups), stand = F) + xlim(-11.2,-4.8) + ylim(-3,3) + ggtitle("Data")
# Creating clustering model
hc_iris <- hclust(distData, method = "complete")
# Groups
groups <- cutree(hc_iris, k = 3)
# plotting the data
fviz_cluster(list(data = scaledData, cluster = groups), stand = F) + xlim(-11.2,-4.8) + ylim(-3,3) + ggtitle("Data")
fviz_cluster(list(data=scaledData, cluster = groups))
hc_iris$labels <- levels(iris$Species)
fviz_cluster(list(data=scaledData, cluster = groups))
# Predicting new data
randomData <- iris[1:4][sample(1:nrow(iris), size = 1),]
predictedData <- knn(train = data, test = randomData, k = 1, cl = groups)
predictedData
predictedData <- knn(train = scaledData, test = randomData, k = 1, cl = groups)
predictedData <- knn(train = scaledData, test = randomData, k = 1, cl = hc_iris)
predictedData <- knn(train = data, test = randomData, k = 1, cl = groups)
predictedData
groups
groups
groups <- hc_iris$labels
# Predicting new data
randomData <- iris[1:4][sample(1:nrow(iris), size = 1),]
predictedData <- knn(train = data, test = randomData, k = 1, cl = groups)
predictedData <- knn(train = data, test = randomData, k = 1)
# Groups
groups <- cutree(hc_iris, k = 3)
hc_iris$labels[predictedData]
randomData
levels(iris$Species)
# labeling data
hc_iris$labels <- levels(iris$Species)
# data
data = iris[1:4]
rm(list = ls())
# data
data = iris[1:4]
# Scaled Data
scaledData <- scale(data)
# Calculating distance
distData <- dist(scaledData, method = "euclidean")
# Row names
rownames(scaledData) <- paste(iris$Species, 1:dim(iris)[1], sep = "_")
# Creating clustering model
hc_iris <- hclust(distData, method = "complete")
# labeling data
hc_iris$labels <- levels(iris$Species)
# Groups
groups <- cutree(hc_iris, k = 3)
# plotting the data
fviz_cluster(list(data = scaledData, cluster = groups), stand = F) + xlim(-11.2,-4.8) + ylim(-3,3) + ggtitle("Data")
# plotting the data
fviz_cluster(list(data = scaledData, cluster = groups))
# Predicting new data
randomData <- iris[1:4][sample(1:nrow(iris), size = 1),]
predictedData <- knn(train = data, test = randomData, k = 1, cl = groups)
hc_iris$labels[predictedData]
randomData
iris[118,]
# Retrieve all the species
iris.labels = iris$Species
# Plotting in the table
table(iris.labels)
# Preparing unlabeled data
iris_data <- iris[1:4]
# Scaling the data
iris_data_scale = scale(iris_data)
# Finding the distance measure using euclidean method
iris_data <- dist(iris_data_scale, method = "euclidean")
# Calculate the number of clusters we need
# wss - within sum of squares, silhouette, gap_stat
fviz_nbclust(iris_data_scale, kmeans, method = "gap_stat") + labs(subtitle = "Elbow method")
# Kmeans
km.out <- kmeans(iris_data_scale, centers = 3)
print(km.out)
# Visualize the clustering algorithm results
clusters = km.out$cluster
rownames(iris_data_scale) <- paste(iris$Species, 1:dim(iris)[1], sep = "_")
fviz_cluster(list(data=iris_data_scale, cluster = clusters))
